#!/bin/bash

# Example commands for knowledge distillation training

echo "==============================================================================="
echo "Knowledge Distillation Training Examples"
echo "==============================================================================="
echo ""

echo "1. Basic distillation with custom teacher checkpoint:"
echo "-----------------------------------------------------"
echo "uv run python train_advanced.py \\"
echo "    --config rgb_hierarchical_unet_v2_distillation_b0_from_b3 \\"
echo "    --teacher_checkpoint experiments/rgb_hierarchical_unet_v2_fullimage_pretrained_peopleseg_r80x60m160x120_disttrans_contdet_baware/checkpoints/checkpoint_epoch_0048.pth \\"
echo "    --epochs 100"
echo ""

echo "2. Quick test with fewer epochs:"
echo "---------------------------------"
echo "uv run python train_advanced.py \\"
echo "    --config rgb_hierarchical_unet_v2_distillation_b0_from_b3 \\"
echo "    --teacher_checkpoint experiments/rgb_hierarchical_unet_v2_fullimage_pretrained_peopleseg_r80x60m160x120_disttrans_contdet_baware/checkpoints/checkpoint_epoch_0048.pth \\"
echo "    --epochs 10"
echo ""

echo "3. Custom distillation parameters:"
echo "-----------------------------------"
echo "uv run python train_advanced.py \\"
echo "    --config rgb_hierarchical_unet_v2_distillation_b0_from_b3 \\"
echo "    --teacher_checkpoint experiments/rgb_hierarchical_unet_v2_fullimage_pretrained_peopleseg_r80x60m160x120_disttrans_contdet_baware/checkpoints/checkpoint_epoch_0048.pth \\"
echo "    --distillation_temperature 6.0 \\"
echo "    --distillation_alpha 0.8 \\"
echo "    --epochs 100"
echo ""

echo "4. Enable distillation on baseline config:"
echo "-------------------------------------------"
echo "uv run python train_advanced.py \\"
echo "    --config baseline \\"
echo "    --teacher_checkpoint experiments/baseline/checkpoints/best_model.pth \\"
echo "    --epochs 50"
echo ""

echo "5. Resume distillation training:"
echo "---------------------------------"
echo "uv run python train_advanced.py \\"
echo "    --config rgb_hierarchical_unet_v2_distillation_b0_from_b3 \\"
echo "    --resume experiments/rgb_hierarchical_unet_v2_distillation_b0_from_b3/checkpoints/checkpoint_epoch_0050.pth \\"
echo "    --teacher_checkpoint experiments/rgb_hierarchical_unet_v2_fullimage_pretrained_peopleseg_r80x60m160x120_disttrans_contdet_baware/checkpoints/checkpoint_epoch_0048.pth \\"
echo "    --epochs 100"
echo ""

echo "6. Using the bash script with parameters:"
echo "------------------------------------------"
echo "./run_distillation_training.sh \\"
echo "    --teacher_checkpoint experiments/rgb_hierarchical_unet_v2_fullimage_pretrained_peopleseg_r80x60m160x120_disttrans_contdet_baware/checkpoints/checkpoint_epoch_0048.pth \\"
echo "    --epochs 50 \\"
echo "    --temperature 5.0 \\"
echo "    --alpha 0.7"
echo ""

echo "==============================================================================="
echo "Notes:"
echo "- Teacher checkpoint must exist and be compatible with the model architecture"
echo "- Temperature controls softness of probability distributions (higher = softer)"
echo "- Alpha controls balance: alpha * distillation_loss + (1-alpha) * ground_truth_loss"
echo "- Student model (B0) has ~6.25M params vs teacher (B3) with ~13.16M params"
echo "==============================================================================="