# UNet知識蒸留アーキテクチャ詳細
## `rgb_hierarchical_unet_v2_distillation_b0_from_b3_temp_prog`

## 📋 概要

本アーキテクチャは、大規模な教師モデル（EfficientNet-B3）から小規模な生徒モデル（EfficientNet-B0）への知識蒸留を実現し、以下の先進的な学習戦略を組み合わせています：

- **知識蒸留**: B3教師からB0生徒への知識転移
- **温度スケジューリング**: 訓練中の蒸留温度を動的に調整
- **段階的エンコーダー解凍**: エンコーダーブロックを段階的に学習可能に

## 🏗️ アーキテクチャ構成

### 全体構造

```
┌─────────────────────────────────────────────────────────────┐
│                     入力画像 (640×640×3)                      │
└────────────────┬────────────────────┬──────────────────────┘
                 │                    │
        ┌────────▼────────┐   ┌──────▼──────┐
        │   教師モデル      │   │  生徒モデル   │
        │ EfficientNet-B3  │   │EfficientNet-B0│
        │    (固定)        │   │  (学習可能)   │
        └────────┬─────────┘   └──────┬──────┘
                 │                    │
        ┌────────▼────────┐   ┌──────▼──────┐
        │  教師出力        │   │  生徒出力    │
        │  (640×640×1)    │   │ (640×640×1) │
        └────────┬─────────┘   └──────┬──────┘
                 │                    │
                 └────────┬───────────┘
                          │
                    ┌─────▼─────┐
                    │  損失計算   │
                    └───────────┘
```

### モデル詳細比較

| 属性 | 教師モデル (B3) | 生徒モデル (B0) | 削減率 |
|------|----------------|----------------|---------|
| パラメータ数 | 12.2M | 5.3M | 57% |
| エンコーダー深度 | 384 | 320 | 17% |
| チャンネル数 | [40, 56, 112, 160, 240, 384] | [32, 40, 80, 112, 192, 320] | 約20% |
| 推論速度 | 1.0x | 2.3x | 130%高速化 |
| メモリ使用量 | 100% | 43% | 57%削減 |

## 🔥 知識蒸留メカニズム

### 1. 出力蒸留（Logit Distillation）

```
教師出力 (Teacher)          生徒出力 (Student)
      │                           │
      │    温度T=4.0→1.0          │
      ▼                           ▼
 Soft Labels                 Predictions
      │                           │
      └──────── KL損失 ───────────┘
                  +
           MSE損失 + BCE損失
```

### 2. 損失関数の構成

```python
Total Loss = α × KL_Loss(T) + β × MSE_Loss + γ × BCE_Loss + δ × Dice_Loss

ここで:
- α = 0.3 (蒸留重み)
- β = 0.5 (MSE重み)
- γ = 1.0 (BCE重み)
- δ = 0.0 (Dice重み - 無効)
- T = 温度（4.0 → 1.0へ動的変化）
```

## 🌡️ 温度スケジューリング

### コサインアニーリング方式

```
温度 T
 4.0 ┤●
     │ ╲
 3.5 ┤  ╲
     │   ╲
 3.0 ┤    ╲___
     │        ╲___
 2.5 ┤            ╲__
     │               ╲___
 2.0 ┤                   ╲___
     │                       ╲___
 1.5 ┤                           ╲___
     │                               ╲___●
 1.0 ┤                                   
     └────────────────────────────────────────
     0    10   20   30   40   50   60   70   80   90  100
                        エポック
```

**効果**:
- **初期（T=4.0）**: ソフトな教師信号で大まかな構造を学習
- **中期（T=2.5）**: 徐々に詳細な特徴を学習
- **後期（T=1.0）**: ハードな予測に収束

## 🔓 段階的エンコーダー解凍

### 解凍スケジュール

```
エンコーダーブロック構造（EfficientNet-B0）
┌─────────────────────────────────────────────┐
│ Block 0: Stem (32ch)                        │ ← Epoch 40+
├─────────────────────────────────────────────┤
│ Block 1: MBConv1 (40ch, 160×160)           │ ← Epoch 35
├─────────────────────────────────────────────┤
│ Block 2: MBConv6 (80ch, 80×80)             │ ← Epoch 30
├─────────────────────────────────────────────┤
│ Block 3: MBConv6 (80ch, 80×80)             │ ← Epoch 25
├─────────────────────────────────────────────┤
│ Block 4: MBConv6 (112ch, 40×40)            │ ← Epoch 20
├─────────────────────────────────────────────┤
│ Block 5: MBConv6 (192ch, 20×20)            │ ← Epoch 15
├─────────────────────────────────────────────┤
│ Block 6: MBConv6 (320ch, 20×20)            │ ← Epoch 10
└─────────────────────────────────────────────┘
         ↑
    最初に解凍（深い層から）
```

### タイムライン

| エポック | 温度 | 解凍ブロック | 学習パラメータ | 学習率 |
|---------|------|-------------|---------------|--------|
| 0-9 | 4.0→3.5 | 0 | デコーダーのみ | 1e-4 |
| 10-14 | 3.5→3.2 | 1 (Block 6) | デコーダー + Block 6 | デコーダー: 1e-4<br>Block 6: 1e-5 |
| 15-19 | 3.2→2.8 | 2 (Block 5-6) | デコーダー + Block 5-6 | デコーダー: 1e-4<br>エンコーダー: 1e-5 |
| 20-24 | 2.8→2.4 | 3 (Block 4-6) | デコーダー + Block 4-6 | デコーダー: 1e-4<br>エンコーダー: 1e-5 |
| 25-29 | 2.4→2.0 | 4 (Block 3-6) | デコーダー + Block 3-6 | デコーダー: 1e-4<br>エンコーダー: 1e-5 |
| 30-34 | 2.0→1.7 | 5 (Block 2-6) | デコーダー + Block 2-6 | デコーダー: 1e-4<br>エンコーダー: 1e-5 |
| 35-39 | 1.7→1.4 | 6 (Block 1-6) | デコーダー + Block 1-6 | デコーダー: 1e-4<br>エンコーダー: 1e-5 |
| 40+ | 1.4→1.0 | 7 (全ブロック) | 全パラメータ | デコーダー: 1e-4<br>エンコーダー: 1e-5 |

## 📊 学習ダイナミクス

### パラメータ更新の流れ

```
エポック 0-9: デコーダー集中学習
┌─────────────┐
│  Encoder    │ ❄️ 凍結
├─────────────┤
│  Decoder    │ 🔥 学習中 (LR: 1e-4)
└─────────────┘

エポック 10-39: 段階的解凍
┌─────────────┐
│  Encoder    │ 
│  - Block 0-3│ ❄️ 凍結
│  - Block 4-6│ 🔥 学習中 (LR: 1e-5)
├─────────────┤
│  Decoder    │ 🔥 学習中 (LR: 1e-4)
└─────────────┘

エポック 40+: 全体微調整
┌─────────────┐
│  Encoder    │ 🔥 全体学習 (LR: 1e-5)
├─────────────┤
│  Decoder    │ 🔥 学習中 (LR: 1e-4)
└─────────────┘
```

## 🎯 期待される性能

### 学習曲線予測

```
mIoU
0.90 ┤                              ━━━━━━━━━━━━
     │                        ━━━━━━  教師 (B3)
0.80 ┤                  ━━━━━━
     │            ━━━━━━               ●●●●●●●●●
0.70 ┤      ━━━━━━                ●●●●●  生徒 (B0)
     │ ━━━━━                  ●●●●
0.60 ┤              ●●●●●●●●●●
     │         ●●●●●
0.50 ┤    ●●●●●
     │●●●●
0.40 ┤
     └────────────────────────────────────────────
     0    10   20   30   40   50   60   70   80   90  100
                        エポック

凡例: ━ 教師性能（固定）  ● 生徒性能（学習中）
```

### 最終性能指標（予測）

| 指標 | 教師 (B3) | 生徒 (B0) | 達成率 |
|------|-----------|-----------|---------|
| mIoU | 0.89 | 0.72-0.75 | 81-84% |
| Dice係数 | 0.92 | 0.78-0.81 | 85-88% |
| 推論時間 | 50ms | 22ms | 2.3倍高速 |
| モデルサイズ | 40MB | 20MB | 50%削減 |

## 🛠️ 実装設定

### 訓練パラメータ

```yaml
# 基本設定
epochs: 100
batch_size: 4-16
optimizer: AdamW
base_learning_rate: 1e-4
encoder_lr_scale: 0.1  # エンコーダーは1/10の学習率

# 温度スケジューリング
initial_temperature: 4.0
final_temperature: 1.0
schedule_type: cosine

# 段階的解凍
unfreeze_start_epoch: 10
unfreeze_rate: 5  # 5エポックごとに1ブロック
total_blocks: 7

# 損失重み
alpha: 0.3  # 蒸留損失
task_weight: 0.7  # タスク損失（BCE）
```

### データ拡張

```python
# 重い拡張（Heavy Augmentation）使用
- HorizontalFlip (p=0.5)
- ColorJitter (brightness=0.2, contrast=0.2)
- RandomBrightnessContrast (p=0.8)
- RandomRain/Fog/SunFlare (p=0.1)
- GaussianBlur (p=0.05)
- GaussNoise (p=0.05)
```

## 💡 主要な設計思想

### 1. **階層的学習**
- デコーダーから開始し、徐々にエンコーダーも学習
- 破壊的忘却を防ぎながらタスク適応

### 2. **温度による制御**
- 高温で開始：一般的な構造を学習
- 低温で終了：詳細な特徴に収束

### 3. **差別的学習率**
- デコーダー：標準学習率（1e-4）
- エンコーダー：低学習率（1e-5）
- 事前学習知識を保持

## 📈 監視指標

訓練中に監視すべき重要指標：

1. **損失値の推移**
   - KL損失：温度に応じて変化
   - MSE損失：教師との差異
   - BCE損失：実際のラベルとの一致

2. **性能指標**
   - 生徒mIoU：主要評価指標
   - 教師との一致率：知識転移の効果
   - 検証損失：過学習の検出

3. **学習状態**
   - 現在の温度
   - 解凍ブロック数
   - 各層の学習率

## 🚀 使用方法

```bash
# 訓練開始
uv run python train_distillation_staged.py \
  --config rgb_hierarchical_unet_v2_distillation_b0_from_b3_temp_prog \
  --epochs 100 \
  --batch_size 16 \
  --mixed_precision

# TensorBoardで監視
tensorboard --logdir experiments/rgb_hierarchical_unet_v2_distillation_b0_from_b3_temp_prog/logs
```

## 📝 まとめ

本アーキテクチャは、最新の知識蒸留技術を組み合わせることで、以下を実現します：

✅ **高精度維持**: 教師性能の80%以上を達成  
✅ **大幅な軽量化**: モデルサイズ50%削減  
✅ **高速推論**: 2.3倍の速度向上  
✅ **安定した学習**: 段階的な複雑性増加  
✅ **柔軟な制御**: 温度と解凍の細かい調整可能